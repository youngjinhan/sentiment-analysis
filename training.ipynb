{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0b768e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     Text sentiment\n",
      "0       I have bought several of the Vitality canned d...  positive\n",
      "1       Product arrived labeled as Jumbo Salted Peanut...  negative\n",
      "2       This is a confection that has been around a fe...  positive\n",
      "3       If you are looking for the secret ingredient i...  negative\n",
      "4       Great taffy at a great price.  There was a wid...  positive\n",
      "...                                                   ...       ...\n",
      "537350  @AmericanAir my flight was Cancelled Flightled...  negative\n",
      "537351         @AmericanAir right on cue with the delaysðŸ‘Œ  negative\n",
      "537352  @AmericanAir thank you we got on a different f...  positive\n",
      "537353  @AmericanAir leaving over 20 minutes Late Flig...  negative\n",
      "537354  @AmericanAir you have my money, you change my ...  negative\n",
      "\n",
      "[537355 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#test_sets = <input data>  ==> string list \n",
    "\n",
    "#data1\n",
    "df = pd.read_csv(\"./Reviews.csv\")\n",
    "df = df[df['Score'] != 3]\n",
    "df['sentiment'] = df['Score'].apply(lambda rating : 'positive' if rating > 3 else 'negative')\n",
    "review_df1=df[['Text','sentiment']]\n",
    "test_sets1=review_df1.Text.values\n",
    "\n",
    "\n",
    "#data2\n",
    "df = pd.read_csv(\"./Tweets.csv\")\n",
    "review_df=df[['text','airline_sentiment']]\n",
    "review_df2 = review_df[review_df['airline_sentiment'] != 'neutral']\n",
    "review_df2.columns=[\"Text\",\"sentiment\"]\n",
    "test_sets2=review_df2.Text.values\n",
    "answers=review_df2.sentiment.values\n",
    "\n",
    "# data concatanation\n",
    "review_df=pd.concat([review_df1,review_df2],ignore_index=True)\n",
    "print(review_df)\n",
    "sentiment_label = review_df.sentiment.factorize()\n",
    "test_sets=review_df.Text.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5c0d922",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(test_sets)\n",
    "# print(tokenizer.word_index, len(tokenizer.word_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fa7eae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_docs = tokenizer.texts_to_sequences(test_sets)\n",
    "# print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "227a979d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0 ...   101    58   141]\n",
      " [    0     0     0 ...    38    24  5778]\n",
      " [    0     0     0 ...     5     1 13888]\n",
      " ...\n",
      " [    0     0     0 ...  1161     5  4306]\n",
      " [    0     0     0 ...   751   753 19091]\n",
      " [    0     0     0 ...    96    13  8104]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "padded_sequence = pad_sequences(encoded_docs, maxlen=200)\n",
    "print(padded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af585189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-30 08:29:20.888851: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2021-11-30 08:29:20.888882: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2021-11-30 08:29:20.889655: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 200, 32)           4231168   \n",
      "                                                                 \n",
      " spatial_dropout1d (SpatialD  (None, 200, 32)          0         \n",
      " ropout1D)                                                       \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 50)                16600     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 50)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,247,819\n",
      "Trainable params: 4,247,819\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense, Dropout, SpatialDropout1D\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "vocab_size=len(tokenizer.word_index)+1\n",
    "embedding_vector_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vector_length, input_length=200))\n",
    "model.add(SpatialDropout1D(0.25))\n",
    "model.add(LSTM(50, dropout=0.5, recurrent_dropout=0.5))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50848612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "840/840 [==============================] - 359s 424ms/step - loss: 0.2284 - accuracy: 0.9122 - val_loss: 0.2521 - val_accuracy: 0.8897\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(padded_sequence,sentiment_label[0],validation_split=0.2, epochs=1, batch_size=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c355ef30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "6717/6717 [==============================] - 2153s 320ms/step - loss: 0.1728 - accuracy: 0.9347 - val_loss: 0.3036 - val_accuracy: 0.8897\n",
      "Epoch 2/6\n",
      "6717/6717 [==============================] - 2152s 320ms/step - loss: 0.1420 - accuracy: 0.9467 - val_loss: 0.2818 - val_accuracy: 0.8960\n",
      "Epoch 3/6\n",
      "6717/6717 [==============================] - 2164s 322ms/step - loss: 0.1222 - accuracy: 0.9550 - val_loss: 0.2790 - val_accuracy: 0.9012\n",
      "Epoch 4/6\n",
      "6717/6717 [==============================] - 2162s 322ms/step - loss: 0.1105 - accuracy: 0.9594 - val_loss: 0.2680 - val_accuracy: 0.9056\n",
      "Epoch 5/6\n",
      "6717/6717 [==============================] - 2160s 322ms/step - loss: 0.1015 - accuracy: 0.9630 - val_loss: 0.2907 - val_accuracy: 0.9054\n",
      "Epoch 6/6\n",
      "6717/6717 [==============================] - 2159s 321ms/step - loss: 0.0949 - accuracy: 0.9656 - val_loss: 0.2887 - val_accuracy: 0.9086\n"
     ]
    }
   ],
   "source": [
    "epochs=6\n",
    "batch_size=64\n",
    "history = model.fit(padded_sequence,sentiment_label[0],validation_split=0.2, epochs=epochs, batch_size=batch_size)\n",
    "model.save('trained_model(epoch='+str(epochs)+',batch_size='+str(batch_size)+').h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9763acc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "13434/13434 [==============================] - 4059s 302ms/step - loss: 0.1969 - accuracy: 0.9244 - val_loss: 0.2622 - val_accuracy: 0.8953\n",
      "Epoch 2/6\n",
      "13434/13434 [==============================] - 4045s 301ms/step - loss: 0.1399 - accuracy: 0.9472 - val_loss: 0.2430 - val_accuracy: 0.9078\n",
      "Epoch 3/6\n",
      "13434/13434 [==============================] - 4040s 301ms/step - loss: 0.1197 - accuracy: 0.9558 - val_loss: 0.2668 - val_accuracy: 0.9074\n",
      "Epoch 4/6\n",
      "13434/13434 [==============================] - 4036s 300ms/step - loss: 0.1091 - accuracy: 0.9602 - val_loss: 0.2642 - val_accuracy: 0.9065\n",
      "Epoch 5/6\n",
      "13434/13434 [==============================] - 4034s 300ms/step - loss: 0.1018 - accuracy: 0.9630 - val_loss: 0.2744 - val_accuracy: 0.9118\n",
      "Epoch 6/6\n",
      "13434/13434 [==============================] - 4019s 299ms/step - loss: 0.0969 - accuracy: 0.9648 - val_loss: 0.2701 - val_accuracy: 0.9112\n"
     ]
    }
   ],
   "source": [
    "epochs=6\n",
    "batch_size=32\n",
    "history = model.fit(padded_sequence,sentiment_label[0],validation_split=0.2, epochs=epochs, batch_size=batch_size)\n",
    "model.save('trained_model(epoch='+str(epochs)+',batch_size='+str(batch_size)+').h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "899a191d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "3359/3359 [==============================] - 1140s 339ms/step - loss: 0.0873 - accuracy: 0.9687 - val_loss: 0.2888 - val_accuracy: 0.9108\n",
      "Epoch 2/7\n",
      "3359/3359 [==============================] - 1141s 340ms/step - loss: 0.0837 - accuracy: 0.9701 - val_loss: 0.2867 - val_accuracy: 0.9133\n",
      "Epoch 3/7\n",
      "3359/3359 [==============================] - 1140s 339ms/step - loss: 0.0805 - accuracy: 0.9709 - val_loss: 0.3047 - val_accuracy: 0.9123\n",
      "Epoch 4/7\n",
      "3359/3359 [==============================] - 1139s 339ms/step - loss: 0.0779 - accuracy: 0.9722 - val_loss: 0.3332 - val_accuracy: 0.9095\n",
      "Epoch 5/7\n",
      "3359/3359 [==============================] - 1139s 339ms/step - loss: 0.0757 - accuracy: 0.9730 - val_loss: 0.3263 - val_accuracy: 0.9099\n",
      "Epoch 6/7\n",
      "3359/3359 [==============================] - 1138s 339ms/step - loss: 0.0733 - accuracy: 0.9739 - val_loss: 0.3161 - val_accuracy: 0.9102\n",
      "Epoch 7/7\n",
      "3359/3359 [==============================] - 1126s 335ms/step - loss: 0.0710 - accuracy: 0.9749 - val_loss: 0.3051 - val_accuracy: 0.9130\n"
     ]
    }
   ],
   "source": [
    "epochs=7\n",
    "batch_size=128\n",
    "history = model.fit(padded_sequence,sentiment_label[0],validation_split=0.2, epochs=epochs, batch_size=batch_size)\n",
    "model.save('trained_model(epoch='+str(epochs)+',batch_size='+str(batch_size)+').h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee1d03e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
